{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fine-tune-bert.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gb1mxiVYsMmR"
      },
      "source": [
        "# Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9v9LnSnuF0fB",
        "outputId": "5de1124f-5dca-4508-8e11-a95aeec2d99b"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sat Jun  5 03:45:35 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 465.27       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   70C    P8    12W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "axlHJcg5RVGc",
        "outputId": "ab130a1a-3207-4976-afe8-3c4433e71ade"
      },
      "source": [
        "!pip3 install transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.6.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n",
            "Requirement already satisfied: huggingface-hub==0.0.8 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.8)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (4.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Jam3BRcRc4j"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset\n",
        "from typing import Counter\n",
        "import numpy as np\n",
        "from keras.preprocessing import sequence\n",
        "from nltk.corpus import gutenberg\n",
        "from string import punctuation\n",
        "import nltk\n",
        "from collections import defaultdict\n",
        "import re\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "from tqdm import tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ks5o0AAAFeIK",
        "outputId": "5fcee645-dd80-4ffa-d4fe-9bfb99d6a66c"
      },
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\", use_fast=False)\n",
        "phobert = AutoModel.from_pretrained(\"vinai/phobert-base\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "Some weights of the model checkpoint at vinai/phobert-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.decoder.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_WPe0ZsSZYai"
      },
      "source": [
        "class MLP_Head(nn.Module):\n",
        "  def __init__(self, input_dim=512, hidden_dim=256, output_dim=4):\n",
        "    super(MLP_Head, self).__init__()\n",
        "    self.net = nn.Sequential( \n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, output_dim)\n",
        "    )\n",
        "    self.softmax = nn.LogSoftmax(dim=-1)\n",
        "\n",
        "  \n",
        "  def forward(self, x):\n",
        "    logits = self.net(x)\n",
        "    return self.softmax(logits)\n",
        "  \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rINOym6G9_GM"
      },
      "source": [
        "class My_Model(nn.Module):\n",
        "  def __init__(self, bert, rnn, head, device):\n",
        "    super(My_Model, self).__init__()\n",
        "\n",
        "    self.bert = bert.to(device)\n",
        "    self.rnn = rnn.to(device)\n",
        "    self.head = head.to(device)\n",
        "    \n",
        "    for param in self.bert.parameters():\n",
        "      param.requires_grad = False\n",
        "\n",
        "  def forward(self, x):\n",
        "\n",
        "    features = self.bert(x)\n",
        "    \n",
        "    hidden, pool = features['last_hidden_state'], features['pooler_output']\n",
        "\n",
        "\n",
        "    feat, _ = self.rnn(hidden)\n",
        "\n",
        "\n",
        "    output = self.head(feat)\n",
        "\n",
        "    return output  \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mwZXCkc-XdJh"
      },
      "source": [
        "class MyDataset(Dataset):\n",
        "    def __init__(self, data_path, label_path, length):\n",
        "       \n",
        "        self.data_path = data_path\n",
        "        self.label_path = label_path\n",
        "        self.length = length\n",
        "\n",
        "        with open(self.data_path, 'r') as f:\n",
        "            sents = f.read().splitlines()\n",
        "\n",
        "        self.sents = [sent.split('<fff>') for sent in sents]\n",
        "        self.sents = [[re.sub(\" \", \"_\", token) for token in line] for line in self.sents]\n",
        "        \n",
        "        self.sents = [['<s>']+line+['</s>'] for line in  self.sents]\n",
        "\n",
        "\n",
        "        self.ids = [tokenizer.convert_tokens_to_ids(line) for line in self.sents]\n",
        "        \n",
        "        self.ids = sequence.pad_sequences(\n",
        "            self.ids, maxlen=self.length, padding=\"post\", value=1)\n",
        "        \n",
        "\n",
        "\n",
        "        with open(self.label_path, 'r') as f:\n",
        "            labels = f.read().splitlines()\n",
        "\n",
        "        \n",
        "\n",
        "        self.labels = [list(map(int, label.split())) for label in labels]\n",
        "        self.labels = [[0]+label+[3] for label in self.labels]\n",
        "\n",
        "        self.labels = sequence.pad_sequences(\n",
        "            self.labels, maxlen=self.length, padding=\"post\", value=3)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        return {'data': self.ids[index], 'label': self.labels[index]}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RbhDjxkbabUT"
      },
      "source": [
        "def dataset_batch_iter(dataset, batch_size):\n",
        "    b_words = []\n",
        "    b_labels = []\n",
        "    for data in dataset:\n",
        "        b_words.append(data['data'])\n",
        "        b_labels.append(data['label'])\n",
        "\n",
        "        if len(b_words) == batch_size:\n",
        "            yield {'data': np.array(b_words, dtype=int), 'label': np.array(b_labels, dtype=int)}\n",
        "            b_words, b_labels = [], []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rH095UA1Y3Bs"
      },
      "source": [
        "train_dataset = MyDataset(data_path='/content/drive/MyDrive/PP/demo_data_hoang/data_text(50k).txt',\n",
        "                    label_path='/content/drive/MyDrive/PP/demo_data_hoang/data_label(50k).txt',\n",
        "                    length=32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6LnyM271C-EL"
      },
      "source": [
        "test_dataset = MyDataset(data_path='/content/drive/MyDrive/PP/demo_data_hoang/test_text(5k).txt',\n",
        "                    label_path='/content/drive/MyDrive/PP/demo_data_hoang/test_label(5k).txt',\n",
        "                    length=32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49DiAWMamJAM"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "rnn = nn.RNN(768, 512, 2, batch_first = True, dropout=0.2, bidirectional=True)\n",
        "head = MLP_Head(512*2, 256, 4)\n",
        "mymodel = My_Model(phobert, rnn, head, device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6oxDGE3DmJ6S"
      },
      "source": [
        "optimizer = torch.optim.Adam(mymodel.parameters())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Two-5-igbsn"
      },
      "source": [
        "nll_loss = nn.NLLLoss()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1MN7DjL4Ieqt"
      },
      "source": [
        "def cal_score(valid_dataset):\n",
        "    mymodel.eval()\n",
        "    print(\"/nTesting\")\n",
        "    correct = 0.\n",
        "    cnf_matrix = np.zeros((4, 4))\n",
        "    for batch, data in tqdm(enumerate(dataset_batch_iter(valid_dataset, 16)),leave=False):\n",
        "      input_tensor = torch.Tensor(data['data']).type(\n",
        "                        torch.LongTensor).to(device)\n",
        "      target_tensor = torch.Tensor(data['label']).type(\n",
        "                        torch.LongTensor).to(device)\n",
        "\n",
        "      output = mymodel(input_tensor)\n",
        "      prediction = output.view(-1, 4).argmax(dim=-1)   \n",
        "      \n",
        "      correct += torch.sum(prediction == target_tensor.view(-1)).item()/(prediction.shape[0])\n",
        "\n",
        "      for t, p in zip(target_tensor.view(-1), prediction):\n",
        "          cnf_matrix[t.cpu().long(), p.cpu().long()] += 1\n",
        "    \n",
        "    correct /= (batch+1)  \n",
        "    accuracy = np.diagonal(cnf_matrix).sum()/cnf_matrix.sum()\n",
        "\n",
        "    precision_1 = cnf_matrix[1][1]/cnf_matrix[:, 1].sum()\n",
        "    recall_1 = cnf_matrix[1][1]/cnf_matrix[1, :].sum()\n",
        "    precision_2 = cnf_matrix[2][2]/cnf_matrix[:, 2].sum()\n",
        "    recall_2 = cnf_matrix[2][2]/cnf_matrix[2, :].sum()\n",
        "    precision_3 = cnf_matrix[3][3]/cnf_matrix[:, 3].sum()\n",
        "    recall_3 = cnf_matrix[3][3]/cnf_matrix[3, :].sum()\n",
        "    precision_0 = cnf_matrix[0][0]/cnf_matrix[:, 0].sum()\n",
        "    recall_0 = cnf_matrix[0][0]/cnf_matrix[0, :].sum()\n",
        "    return accuracy,  precision_0, recall_0, precision_1, recall_1, precision_2, recall_2, precision_3, recall_3\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bMafNmLbHdQj"
      },
      "source": [
        "def load():\n",
        "  path = '/content/drive/MyDrive/QA_Bert/bertPP_big.pt'\n",
        "  \n",
        "  try:\n",
        "    checkpoint = torch.load(path)\n",
        "    mymodel.load_state_dict(checkpoint['model_state_dict'])  \n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    print(\"Load successfully\")\n",
        "  except:\n",
        "    print(\"Load fail\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ux3zTUffaqmQ",
        "outputId": "918e5004-046a-4b9a-f27e-c4c7b7f84c27"
      },
      "source": [
        "for epoch in range(10):\n",
        "  mymodel.train()\n",
        "  epoch_loss = 0.\n",
        "  for batch, data in tqdm(enumerate(dataset_batch_iter(train_dataset, 16)), leave=False):\n",
        "    input_tensor = torch.Tensor(data['data']).type(\n",
        "                      torch.LongTensor).to(device)\n",
        "    target_tensor = torch.Tensor(data['label']).type(\n",
        "                      torch.LongTensor).to(device)\n",
        "\n",
        "    output = mymodel(input_tensor)\n",
        "    optimizer.zero_grad()\n",
        "    loss = nll_loss(output.view(-1, 4), target_tensor.view(-1))\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    epoch_loss += loss.item()\n",
        "  \n",
        "  acc = cal_score(test_dataset)\n",
        "\n",
        "  print(f\"\\nEpoch {epoch} with loss {epoch_loss} \\nand test acc {acc}\")\n",
        "\n",
        "  if epoch % 1 == 0:\n",
        "    path = '/content/drive/MyDrive/QA_Bert/bertPP_big.pt'\n",
        "    torch.save(\n",
        "        {\n",
        "            'model_state_dict': mymodel.state_dict(),\n",
        "            'optimizer_state_dict':optimizer.state_dict(),\n",
        "        }, path            \n",
        "    )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3it [00:00, 21.88it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "/nTesting\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            ""
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 0 with loss 429.71626645326614 \n",
            "and test acc (0.9548590244391025, 0.9566813373770249, 0.9771731328623777, 0.7729058189458299, 0.649148381156974, 0.9920979849861715, 0.9578485599847416, 1.0, 1.0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "3it [00:00, 22.30it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "/nTesting\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            ""
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 1 with loss 382.24288139119744 \n",
            "and test acc (0.9562800480769231, 0.9538935553680813, 0.982433606588342, 0.807975338106603, 0.6233696486113243, 0.9950524440926183, 0.9589929429715811, 1.0, 1.0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "3it [00:00, 21.94it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "/nTesting\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            ""
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 2 with loss 372.87676361203194 \n",
            "and test acc (0.9570062099358975, 0.9564436502965774, 0.9807241887348419, 0.7995630284031537, 0.6457725947521866, 0.9944674965421854, 0.9599465954606141, 0.9999718974820144, 1.0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "3it [00:00, 22.11it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "/nTesting\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            ""
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 3 with loss 365.01013465225697 \n",
            "and test acc (0.9573004306891025, 0.96075716462848, 0.976436477654792, 0.7766652154984762, 0.6843639711523707, 0.9936796365791033, 0.9595651344650009, 0.9999718974820144, 1.0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "3it [00:00, 22.39it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "/nTesting\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            ""
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 4 with loss 359.6585277058184 \n",
            "and test acc (0.9582832532051282, 0.958039313353811, 0.9809036303879718, 0.8044028103044496, 0.6588154058615927, 0.9946808510638298, 0.9629982834255197, 1.0, 0.9999718966922406)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "3it [00:00, 22.09it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "/nTesting\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            ""
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 5 with loss 358.20995550602674 \n",
            "and test acc (0.9585023537660257, 0.9609713548674456, 0.9780608968304938, 0.7885022960084775, 0.6850544729169863, 0.9944773175542406, 0.9616631699408735, 1.0, 1.0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "3it [00:00, 22.05it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "/nTesting\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            ""
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 6 with loss 355.7786992266774 \n",
            "and test acc (0.9579952924679487, 0.9608635097493036, 0.9773431302179744, 0.7835486422356973, 0.6840570814792082, 0.9937082186394023, 0.9639519359145527, 1.0, 0.9998875867689627)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "3it [00:00, 21.48it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "/nTesting\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            ""
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 7 with loss 355.3194943368435 \n",
            "and test acc (0.958320813301282, 0.9598848030817954, 0.9789675493936761, 0.7931376080691642, 0.6756943378855301, 0.994277821625888, 0.9610909784474537, 1.0, 1.0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "3it [00:00, 21.98it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "/nTesting\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            ""
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 8 with loss 354.9091358035803 \n",
            "and test acc (0.9584460136217948, 0.9620042413870079, 0.9767953609610517, 0.7813604621088025, 0.6953352769679301, 0.9966336633663366, 0.9599465954606141, 1.0, 1.0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "3it [00:00, 21.89it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "/nTesting\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            ""
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 9 with loss 356.1103294044733 \n",
            "and test acc (0.9581956129807693, 0.9590027547190741, 0.9797797589815269, 0.796999359619431, 0.6684057081479208, 0.9954464462482677, 0.9589929429715811, 0.9999718974820144, 1.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qRYk8I9ZQiGv"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}